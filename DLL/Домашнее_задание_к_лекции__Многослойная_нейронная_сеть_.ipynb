{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Цель задания**: научиться на простейшем уровне подбирать архитектуру полносвязной нейронной сети для получения качества решения задачи не ниже заданного.\n",
        "\n",
        "**Задание**\n",
        "\n",
        "Постройте модель на основе полносвязных слоёв для классификации Fashion MNIST из библиотеки torchvision (datasets).\n",
        "Получите качество на тестовой выборке не ниже 88%\n",
        "\n",
        "**Инструкция по выполнению задания**\n",
        "\n",
        "Скачайте тренировочную и тестовою часть датасета\n",
        "Постройте модель, выбрав стартовую архитектуру\n",
        "Обучите модель и сверьте качество на тестовой части с заданным порогом\n",
        "Изменяйте архитектуру модели пока качество на тестовой части не будет выше порога. Вариации архитектуры можно реализовать через изменение количества слоёв, количества нейронов в слоях и использование регуляризации. Можно использовать различные оптимизаторы."
      ],
      "metadata": {
        "id": "PWdll99eZ_BI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "1t8Ma_Abbbgp"
      },
      "outputs": [],
      "source": [
        "import torchvision as tv\n",
        "import torch\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "f_mnist_train = tv.datasets.FashionMNIST('.', train=True, transform=tv.transforms.ToTensor(), download=True)\n",
        "f_mnist_test  = tv.datasets.FashionMNIST('.', train=False, transform=tv.transforms.ToTensor(), download=True)"
      ],
      "metadata": {
        "id": "NXzx3gGkbpjx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bbdef2df-7ed7-4d95-883a-0f6683d8ed87"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to ./FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 26421880/26421880 [00:03<00:00, 7054973.73it/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./FashionMNIST/raw/train-images-idx3-ubyte.gz to ./FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to ./FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 29515/29515 [00:00<00:00, 116064.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./FashionMNIST/raw/train-labels-idx1-ubyte.gz to ./FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to ./FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4422102/4422102 [00:02<00:00, 2196885.42it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./FashionMNIST/raw/t10k-images-idx3-ubyte.gz to ./FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to ./FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5148/5148 [00:00<00:00, 16961725.84it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to ./FashionMNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "f_mnist_train[0][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DenwLICvbwu5",
        "outputId": "918dca9a-e9cf-4fdd-ac0f-75eb45c7bc6a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000, 0.0039, 0.0000, 0.0000, 0.0510,\n",
              "          0.2863, 0.0000, 0.0000, 0.0039, 0.0157, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0039, 0.0039, 0.0000],\n",
              "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000, 0.0118, 0.0000, 0.1412, 0.5333,\n",
              "          0.4980, 0.2431, 0.2118, 0.0000, 0.0000, 0.0000, 0.0039, 0.0118,\n",
              "          0.0157, 0.0000, 0.0000, 0.0118],\n",
              "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000, 0.0235, 0.0000, 0.4000, 0.8000,\n",
              "          0.6902, 0.5255, 0.5647, 0.4824, 0.0902, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0471, 0.0392, 0.0000],\n",
              "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.6078, 0.9255,\n",
              "          0.8118, 0.6980, 0.4196, 0.6118, 0.6314, 0.4275, 0.2510, 0.0902,\n",
              "          0.3020, 0.5098, 0.2824, 0.0588],\n",
              "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0039, 0.0000, 0.2706, 0.8118, 0.8745,\n",
              "          0.8549, 0.8471, 0.8471, 0.6392, 0.4980, 0.4745, 0.4784, 0.5725,\n",
              "          0.5529, 0.3451, 0.6745, 0.2588],\n",
              "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0039, 0.0039, 0.0039, 0.0000, 0.7843, 0.9098, 0.9098,\n",
              "          0.9137, 0.8980, 0.8745, 0.8745, 0.8431, 0.8353, 0.6431, 0.4980,\n",
              "          0.4824, 0.7686, 0.8980, 0.0000],\n",
              "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.7176, 0.8824, 0.8471,\n",
              "          0.8745, 0.8941, 0.9216, 0.8902, 0.8784, 0.8706, 0.8784, 0.8667,\n",
              "          0.8745, 0.9608, 0.6784, 0.0000],\n",
              "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.7569, 0.8941, 0.8549,\n",
              "          0.8353, 0.7765, 0.7059, 0.8314, 0.8235, 0.8275, 0.8353, 0.8745,\n",
              "          0.8627, 0.9529, 0.7922, 0.0000],\n",
              "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0039, 0.0118, 0.0000, 0.0471, 0.8588, 0.8627, 0.8314,\n",
              "          0.8549, 0.7529, 0.6627, 0.8902, 0.8157, 0.8549, 0.8784, 0.8314,\n",
              "          0.8863, 0.7725, 0.8196, 0.2039],\n",
              "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0235, 0.0000, 0.3882, 0.9569, 0.8706, 0.8627,\n",
              "          0.8549, 0.7961, 0.7765, 0.8667, 0.8431, 0.8353, 0.8706, 0.8627,\n",
              "          0.9608, 0.4667, 0.6549, 0.2196],\n",
              "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0157, 0.0000, 0.0000, 0.2157, 0.9255, 0.8941, 0.9020,\n",
              "          0.8941, 0.9412, 0.9098, 0.8353, 0.8549, 0.8745, 0.9176, 0.8510,\n",
              "          0.8510, 0.8196, 0.3608, 0.0000],\n",
              "         [0.0000, 0.0000, 0.0039, 0.0157, 0.0235, 0.0275, 0.0078, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000, 0.9294, 0.8863, 0.8510, 0.8745,\n",
              "          0.8706, 0.8588, 0.8706, 0.8667, 0.8471, 0.8745, 0.8980, 0.8431,\n",
              "          0.8549, 1.0000, 0.3020, 0.0000],\n",
              "         [0.0000, 0.0118, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.2431, 0.5686, 0.8000, 0.8941, 0.8118, 0.8353, 0.8667,\n",
              "          0.8549, 0.8157, 0.8275, 0.8549, 0.8784, 0.8745, 0.8588, 0.8431,\n",
              "          0.8784, 0.9569, 0.6235, 0.0000],\n",
              "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0706, 0.1725, 0.3216, 0.4196,\n",
              "          0.7412, 0.8941, 0.8627, 0.8706, 0.8510, 0.8863, 0.7843, 0.8039,\n",
              "          0.8275, 0.9020, 0.8784, 0.9176, 0.6902, 0.7373, 0.9804, 0.9725,\n",
              "          0.9137, 0.9333, 0.8431, 0.0000],\n",
              "         [0.0000, 0.2235, 0.7333, 0.8157, 0.8784, 0.8667, 0.8784, 0.8157,\n",
              "          0.8000, 0.8392, 0.8157, 0.8196, 0.7843, 0.6235, 0.9608, 0.7569,\n",
              "          0.8078, 0.8745, 1.0000, 1.0000, 0.8667, 0.9176, 0.8667, 0.8275,\n",
              "          0.8627, 0.9098, 0.9647, 0.0000],\n",
              "         [0.0118, 0.7922, 0.8941, 0.8784, 0.8667, 0.8275, 0.8275, 0.8392,\n",
              "          0.8039, 0.8039, 0.8039, 0.8627, 0.9412, 0.3137, 0.5882, 1.0000,\n",
              "          0.8980, 0.8667, 0.7373, 0.6039, 0.7490, 0.8235, 0.8000, 0.8196,\n",
              "          0.8706, 0.8941, 0.8824, 0.0000],\n",
              "         [0.3843, 0.9137, 0.7765, 0.8235, 0.8706, 0.8980, 0.8980, 0.9176,\n",
              "          0.9765, 0.8627, 0.7608, 0.8431, 0.8510, 0.9451, 0.2549, 0.2863,\n",
              "          0.4157, 0.4588, 0.6588, 0.8588, 0.8667, 0.8431, 0.8510, 0.8745,\n",
              "          0.8745, 0.8784, 0.8980, 0.1137],\n",
              "         [0.2941, 0.8000, 0.8314, 0.8000, 0.7569, 0.8039, 0.8275, 0.8824,\n",
              "          0.8471, 0.7255, 0.7725, 0.8078, 0.7765, 0.8353, 0.9412, 0.7647,\n",
              "          0.8902, 0.9608, 0.9373, 0.8745, 0.8549, 0.8314, 0.8196, 0.8706,\n",
              "          0.8627, 0.8667, 0.9020, 0.2627],\n",
              "         [0.1882, 0.7961, 0.7176, 0.7608, 0.8353, 0.7725, 0.7255, 0.7451,\n",
              "          0.7608, 0.7529, 0.7922, 0.8392, 0.8588, 0.8667, 0.8627, 0.9255,\n",
              "          0.8824, 0.8471, 0.7804, 0.8078, 0.7294, 0.7098, 0.6941, 0.6745,\n",
              "          0.7098, 0.8039, 0.8078, 0.4510],\n",
              "         [0.0000, 0.4784, 0.8588, 0.7569, 0.7020, 0.6706, 0.7176, 0.7686,\n",
              "          0.8000, 0.8235, 0.8353, 0.8118, 0.8275, 0.8235, 0.7843, 0.7686,\n",
              "          0.7608, 0.7490, 0.7647, 0.7490, 0.7765, 0.7529, 0.6902, 0.6118,\n",
              "          0.6549, 0.6941, 0.8235, 0.3608],\n",
              "         [0.0000, 0.0000, 0.2902, 0.7412, 0.8314, 0.7490, 0.6863, 0.6745,\n",
              "          0.6863, 0.7098, 0.7255, 0.7373, 0.7412, 0.7373, 0.7569, 0.7765,\n",
              "          0.8000, 0.8196, 0.8235, 0.8235, 0.8275, 0.7373, 0.7373, 0.7608,\n",
              "          0.7529, 0.8471, 0.6667, 0.0000],\n",
              "         [0.0078, 0.0000, 0.0000, 0.0000, 0.2588, 0.7843, 0.8706, 0.9294,\n",
              "          0.9373, 0.9490, 0.9647, 0.9529, 0.9569, 0.8667, 0.8627, 0.7569,\n",
              "          0.7490, 0.7020, 0.7137, 0.7137, 0.7098, 0.6902, 0.6510, 0.6588,\n",
              "          0.3882, 0.2275, 0.0000, 0.0000],\n",
              "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1569,\n",
              "          0.2392, 0.1725, 0.2824, 0.1608, 0.1373, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000]]])"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train, test = torch.utils.data.DataLoader(f_mnist_train, 6000, shuffle=True), torch.utils.data.DataLoader(f_mnist_test, 1000, shuffle=True)"
      ],
      "metadata": {
        "id": "CcTzaSvNb8l6"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for X, y in train:\n",
        "    print(X.shape, y.shape)\n",
        "    break\n",
        "\n",
        "for X, y in test:\n",
        "    print(X.shape, y.shape)\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nbkWTf8ucCqR",
        "outputId": "847e95c8-82c1-4a22-f3ba-1d5995ae1f1e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([6000, 1, 28, 28]) torch.Size([6000])\n",
            "torch.Size([1000, 1, 28, 28]) torch.Size([1000])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "LR = 0.01\n",
        "\n",
        "model = torch.nn.Sequential(\n",
        "    torch.nn.Flatten(),\n",
        "    torch.nn.Linear(784, 512),\n",
        "    torch.nn.ReLU(),\n",
        "    torch.nn.Linear(512, 256),\n",
        "    torch.nn.ReLU(),\n",
        "    torch.nn.Linear(256, 128),\n",
        "    torch.nn.ReLU(),\n",
        "    torch.nn.Linear(128, 10)\n",
        ")\n",
        "\n",
        "\n",
        "loss = torch.nn.CrossEntropyLoss(reduction='sum')\n",
        "trainer = torch.optim.Adam(model.parameters(), LR)"
      ],
      "metadata": {
        "id": "ygcWbAaR4FPZ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_EPOCHS = 20\n",
        "\n",
        "for epoch in range(1, NUM_EPOCHS+1):\n",
        "\n",
        "    start=time.time()\n",
        "    train_loss, train_acc, train_n = 0., 0., 0\n",
        "    test_loss, test_acc, test_n = 0., 0., 0\n",
        "    \n",
        "\n",
        "    model.train()\n",
        "    for X, y in train:\n",
        "        trainer.zero_grad()\n",
        "        y_pred = model(X)\n",
        "        l = loss(y_pred, y)\n",
        "        l.backward()\n",
        "        trainer.step()\n",
        "        train_loss += l.item()\n",
        "        train_acc += (y_pred.argmax(dim=1) == y).sum().item()\n",
        "        train_n += len(X)\n",
        "    \n",
        "    model.eval()\n",
        "    for X, y in test:\n",
        "        y_pred = model(X)\n",
        "        l = loss(y_pred, y)\n",
        "        test_loss += l.item()\n",
        "        test_acc += (y_pred.argmax(dim=1) == y).sum().item()\n",
        "        test_n += len(X)\n",
        "        \n",
        "    print(\"\\nepoch: {}, taked: {:.2f}, train_loss: {:.3f}, train_acc: {:.2f}%, test_loss: {:.3f}, test_acc: {:.2f}%\".format(\n",
        "        epoch, time.time() - start, train_loss / train_n, train_acc / train_n *100 , test_loss / test_n, test_acc / test_n *100))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pje3w8m9cIN7",
        "outputId": "fbf48f0a-5133-4810-aa43-4e7e90b3df66"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "epoch: 1, taked: 14.09, train_loss: 1.874, train_acc: 39.52%, test_loss: 1.008, test_acc: 59.35%\n",
            "\n",
            "epoch: 2, taked: 14.18, train_loss: 0.872, train_acc: 65.95%, test_loss: 0.743, test_acc: 71.48%\n",
            "\n",
            "epoch: 3, taked: 14.26, train_loss: 0.641, train_acc: 76.17%, test_loss: 0.585, test_acc: 77.56%\n",
            "\n",
            "epoch: 4, taked: 13.85, train_loss: 0.517, train_acc: 81.10%, test_loss: 0.494, test_acc: 82.05%\n",
            "\n",
            "epoch: 5, taked: 17.01, train_loss: 0.447, train_acc: 83.66%, test_loss: 0.453, test_acc: 83.37%\n",
            "\n",
            "epoch: 6, taked: 14.37, train_loss: 0.401, train_acc: 85.26%, test_loss: 0.416, test_acc: 84.58%\n",
            "\n",
            "epoch: 7, taked: 13.77, train_loss: 0.378, train_acc: 85.93%, test_loss: 0.400, test_acc: 85.58%\n",
            "\n",
            "epoch: 8, taked: 13.94, train_loss: 0.353, train_acc: 86.89%, test_loss: 0.390, test_acc: 85.53%\n",
            "\n",
            "epoch: 9, taked: 14.13, train_loss: 0.335, train_acc: 87.59%, test_loss: 0.392, test_acc: 85.81%\n",
            "\n",
            "epoch: 10, taked: 13.75, train_loss: 0.322, train_acc: 88.12%, test_loss: 0.357, test_acc: 87.13%\n",
            "\n",
            "epoch: 11, taked: 13.98, train_loss: 0.313, train_acc: 88.44%, test_loss: 0.366, test_acc: 86.83%\n",
            "\n",
            "epoch: 12, taked: 13.78, train_loss: 0.301, train_acc: 88.85%, test_loss: 0.359, test_acc: 87.18%\n",
            "\n",
            "epoch: 13, taked: 14.21, train_loss: 0.289, train_acc: 89.22%, test_loss: 0.351, test_acc: 87.45%\n",
            "\n",
            "epoch: 14, taked: 14.31, train_loss: 0.276, train_acc: 89.76%, test_loss: 0.351, test_acc: 87.55%\n",
            "\n",
            "epoch: 15, taked: 13.77, train_loss: 0.273, train_acc: 89.72%, test_loss: 0.346, test_acc: 87.81%\n",
            "\n",
            "epoch: 16, taked: 13.87, train_loss: 0.262, train_acc: 90.12%, test_loss: 0.340, test_acc: 88.13%\n",
            "\n",
            "epoch: 17, taked: 13.76, train_loss: 0.252, train_acc: 90.68%, test_loss: 0.338, test_acc: 88.11%\n",
            "\n",
            "epoch: 18, taked: 13.77, train_loss: 0.246, train_acc: 90.79%, test_loss: 0.335, test_acc: 88.21%\n",
            "\n",
            "epoch: 19, taked: 13.89, train_loss: 0.239, train_acc: 91.16%, test_loss: 0.335, test_acc: 88.45%\n",
            "\n",
            "epoch: 20, taked: 13.77, train_loss: 0.228, train_acc: 91.50%, test_loss: 0.333, test_acc: 88.27%\n"
          ]
        }
      ]
    }
  ]
}